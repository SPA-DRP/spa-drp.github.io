---
layout: default
title:  'Mentors and Project Descriptions: Autumn 2021'
---


<div class="col-md-8">
  <p class="lead">
    The application for Autumn 2021 will be
    open from September 6 through September 17th. For Autumn 2021, mentor/mentee pairs may choose whether they would like to meet
    virtually or meet in person.
  Please feel free to <a href=https://spa-drp.github.io/contact.html> contact </a> the student organizers with
    any questions.
  </p>
</div>

<h4> Nick Irons: Introduction to Bayesian Data Analysis</h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i>
    Knowledge of expectations and probability distributions at the level of STAT 340-341 and some knowledge of R.
</li>
<li style="padding:0px;margin:0px 20px 0px;">
  Bayesian statistics is a method of modeling data that synthesizes our prior beliefs about the data with the information contained in the sample to estimate model parameters. Rather than a single point estimate of a parameter, the output of a Bayesian model is a "posterior" distribution which captures the uncertainty in our inferences. Bayesian methods are at the heart of many modern data science and machine learning techniques. In this introduction to Bayesian statistics we will cover conditional distributions, Bayes' theorem, basics of Bayesian modeling, conjugate priors, MCMC sampling, and application to real dataset(s) of interest to the student in R. If time permits, possible further directions include hypothesis testing, linear regression, hierarchical models, and the EM algorithm for missing data. The goal of this project is to come away with an understanding of the basic conceptual and technical aspects of Bayesian inference and to get our hands dirty with real and interesting data. Possible data applications include estimating (potentially waning) COVID vaccine efficacy, estimating COVID prevalence over time in Washington state, or any other dataset of interest to the student.
</li>
</p>

<h4> Alex Ziyu Jiang:   Clustering and music genre classification </h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: Knowledge of probability at the level of Stat 311 or beyond; Some coding experiences, preferably in Python or R; It will be fantastic if you also happen to like listening to music ;)</i>

</li>
<li style="padding:0px;margin:0px 20px 0px;">
  Have you ever been amazed by the sheer amount of music genres in your Spotify or Apple Music App and would like to know about their differences in a quantitative way? In this project you will learn how to process audio data and use some interesting clustering techniques in machine learning to classify songs into different genres.
</li>
</p>

<h4> David Marcano and Daniel Suen:  Cluster Analysis </h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: Basic knowledge of R or Python, statistical background equivalent to STAT 311 is recommended
</i>

</li>
<li style="padding:0px;margin:0px 20px 0px;">
  In many real-world data applications, from medicine to finance, it is of interest to find groups within the data. Clustering is an unsupervised learning approach for separating data into representative groups. How to find and assess the quality of these discovered clusters is a vast area of modern research. In this project, we will survey several popular clustering techniques and utilize them in simulated and real datasets. In particular, we will explore center-based approaches such as the k-means algorithm, dissimilarity-based approaches such as hierarchical clustering, probability-based approaches such as mixture models, and other techniques based on student interest. We will also look at how to assess a given clustering. The topics covered and their depth will develop based on the interest and statistical/mathematical level of the student. We are happy to take two students if more than one person is interested in this project.
</li>
</p>

<h4> Anna Neufeld: Multiple Testing </h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites:
      Stat 311 and some knowledge of R will be helpful, but not required.
    </i>

</li>
In an introductory statistics course, you learn how to obtain a p-value to test a single null hypothesis. These p-values are constructed such that, when the null hypothesis is true, you will make a mistake and reject the null only 5% of the time. In the real world, scientists often wish to test thousands of null hypotheses at once. In this setting, making a mistake on 5% of the hypotheses could lead to a very high number of false discoveries. Multiple testing techniques aim to limit the number of mistakes made over a large set of hypotheses without sacrificing too much power. We will start with a review of hypothesis testing, then discuss the challenges posed by large numbers of hypotheses, and finally learn about modern multiple testing techniques. Towards the end of the quarter, we will apply the techniques we learned to real data.
<li style="padding:0px;margin:0px 20px 0px;">
</li>
</p>

<h4> Michael Pearce: Voting, Ranking, and Preference Modeling
</h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i>
Stat 311 or equivalent
</li>
<li style="padding:0px;margin:0px 20px 0px;">
  Preference data appears in many forms: voters deciding between candidates in an election, movie critics rating new releases, and search engines ranking web pages, to name a few! However, modeling preferences in a statistical manner can be challenging for a variety of reasons, such as computational difficulties in working with discrete and high-dimensional data. In this project, we will study a variety of models used for preference data, which includes both ranking and scoring models. Understanding challenges and uncertainty in aggregating preferences will be a key focus. Together, we will also carry out an applied project on preference data based on the student's interests.
</li>
</p>

<h4> Seth Temple: Statistical Genetics I, Pedigrees and Relatedness </h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i>
      STAT 311; some programming experience preferred
</li>
<li style="padding:0px;margin:0px 20px 0px;">
We will explore statistical theory and methodology as it applies to the study of (human) heredity. The overarching theme of the readings are (1) to compute measures of relatedness (kinship and inbreeding) and conditional trait (disease) risk based on known family trees and (2) to estimate relatedness given dense SNP or entire genome sequence data. Readings will follow UW emeritus professor Elizabeth Thompson’s monograph “Statistical Inference from Genetic on Pedigrees”. We will cover conditional probabilities, likelihood models, Hardy-Weinberg equilibrium, the expectation-maximization algorithm to infer allele frequencies for the ABO blood group, Wright’s path counting formula, and identity by descent. During meetings we will work through practice exercises; for 1 or 2 meetings we will go through brief hands-on labs using current research software. The final project may involve estimating familial relationships among individuals in the 1000 Genomes database and comparing outputs among various statistical software.
</li>
</p>

<h4> Vydhourie R.T. Thiyageswaran: Graph Clustering </h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i>
      Introductory Linear Algebra (and interest in basic introductory graph theory would be helpful)
</li>
<li style="padding:0px;margin:0px 20px 0px;">
We will explore clustering methods in graphs. We will focus on k-means clustering, and spectral clustering. Additionally, we would spend some time looking at applications, by thinking about studies explored in statistical blog entries, for example, in FiveThirtyEight. If there’s interest, we can look into replicating and extending on some of the ideas in these studies.
</li>
</p>

<h4> Kenny Zhang: Basics of Causal Inference</h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i>
      STAT 311 level statistics, some familarity with regression is a plus.
</li>
<li style="padding:0px;margin:0px 20px 0px;">
"Correlation is not causation" used to prevent statisticians from answering questions like "Will smoking cause Lung cancer?". However, with the tool of causal inference and the emergence of big data, we are able to answer some of the questions on a firm scientific basis. We can use causal inference to look at a variety of topics including vaccination, genes etc.
</li>
</p>
